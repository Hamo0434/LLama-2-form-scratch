{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNtURZR5hJG7tdWYI0ZX81Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hamo0434/LLama-2-form-scratch/blob/main/fine_tuned_model_falcon.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9wxa1Gez638B"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig, TrainingArguments, Trainer\n",
        "import time\n",
        "# %pip install evaluate\n",
        "import evaluate\n",
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U datasets huggingface_hub fsspec"
      ],
      "metadata": {
        "id": "iQaEhZ7JXTTR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install -U datasets huggingface_hub fsspec"
      ],
      "metadata": {
        "id": "q1UbBQ_GrHK5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"knkarthick/dialogsum\")"
      ],
      "metadata": {
        "id": "Fg5bwmCEpwLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['HF_HUB_DOWNLOAD_TIMEOUT'] = '60'\n",
        "\n",
        "model_name = 'google/flan-t5-base'\n",
        "original_model = AutoModelForSeq2SeqLM.from_pretrained(model_name ,torch_dtype = torch.bfloat16)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "FgCa55hDqL6N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_number_of_model_trained_parameters(model):\n",
        "  all_parameters = 0\n",
        "  trained_parameters = 0\n",
        "  for _,parameter in model.named_parameters():\n",
        "    all_parameters += parameter.numel()\n",
        "    if parameter.requires_grad:\n",
        "      trained_parameters += parameter.numel()\n",
        "  return (f\"trained_params {trained_parameters} \\nall model parameters: {all_parameters}\\npercentage of trainable model parameters: {100 * trained_parameters / all_parameters:.2f}%\")\n",
        "\n",
        "print(print_number_of_model_trained_parameters(original_model))"
      ],
      "metadata": {
        "id": "jqm-eFMyts9U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index = 150\n",
        "dialogue = dataset['test'][index]['dialogue']\n",
        "\n",
        "summary = dataset['test'][index]['summary']\n",
        "\n",
        "prompt = f''' summarize the following text'\n",
        "{dialogue}\n",
        "summary :\n",
        "'''\n",
        "inputs = tokenizer(prompt , return_tensors = 'pt')\n",
        "output = tokenizer.decode(\n",
        "      original_model.generate(\n",
        "        inputs['input_ids'],\n",
        "        max_new_tokens = 150,\n",
        "    )[0],\n",
        "    skip_special_tokens = True\n",
        ")\n",
        "dash_line = '-'.join('' for x in range(100))\n",
        "print(dash_line)\n",
        "print(f'INPUT PROMPT:\\n{prompt}')\n",
        "print(dash_line)\n",
        "print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n",
        "print(dash_line)\n",
        "print(f'MODEL GENERATION - ZERO SHOT:\\n{output}')"
      ],
      "metadata": {
        "id": "4LSPEKhdugNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function(example):\n",
        "  start_prompt = 'summarize this text'\n",
        "  end_prompt = '\\n Summary'\n",
        "  prompt = [start_prompt + dialogue + end_prompt for dialogue in example['dialogue']]\n",
        "  example['input_ids'] = tokenizer(prompt , padding = 'max_length' , truncation = True , return_tensors = 'pt').input_ids\n",
        "  example['labels'] = tokenizer(example['summary'] , padding= 'max_length' , truncation = True , return_tensors = 'pt').input_ids\n",
        "  return example\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize_function , batched = True)\n",
        "tokenized_dataset = tokenized_dataset.remove_columns(['topic' , 'id' , 'dialogue' , 'summary'])"
      ],
      "metadata": {
        "id": "3h6yXrgxxpAg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## checking the shapes of the data"
      ],
      "metadata": {
        "id": "8fWtmyysZdsS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('The shapes of datasets :')\n",
        "print(f\"Training set : {tokenized_dataset['train'].shape}\")\n",
        "print(f\"Testing set : {tokenized_dataset['test'].shape}\")\n",
        "print(f\"Validation set : {tokenized_dataset['validation'].shape}\")"
      ],
      "metadata": {
        "id": "1gOmnu2YZFbx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fine Tune the model in the new dataset"
      ],
      "metadata": {
        "id": "ITugUQFXcwlC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_dir = f'./dialogue-summary-training-{str(int(time.time()))}'\n",
        "training_arguments = TrainingArguments(\n",
        "    output_dir = output_dir ,\n",
        "    num_train_epochs = 1 ,\n",
        "    learning_rate = 1e-5 ,\n",
        "    weight_decay = 0.01 ,\n",
        "    logging_steps = 1 ,\n",
        "    max_steps = 1\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model = original_model ,\n",
        "    args = training_arguments ,\n",
        "    train_dataset = tokenized_dataset['train'] ,\n",
        "    eval_dataset = tokenized_dataset['test']\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "brBEHrTdcpRH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Pc-rKvfWfuin"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we are already to fine tune the model\n"
      ],
      "metadata": {
        "id": "0X-ZnoaqfwRO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "PkpEgRt3fvWc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Sne3FOoJVkm7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save the model\n"
      ],
      "metadata": {
        "id": "enaN5SethRFG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trained_model_dir = \"./trained_model\"\n",
        "trainer.save_model(trained_model_dir)\n",
        "\n",
        "trained_model = AutoModelForSeq2SeqLM.from_pretrained(trained_model_dir)"
      ],
      "metadata": {
        "id": "9TxQHtDHhPg3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation of the model"
      ],
      "metadata": {
        "id": "dfiuLwoZhqZ4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "08ob5DcJYkzX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Zx4pK0FJYkx7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = tokenizer(prompt , return_tensors = 'pt').input_ids\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "trained_model = trained_model.to(device)\n",
        "original_model = original_model.to(device)\n",
        "input_ids = input_ids.to(device)\n",
        "\n",
        "# Generate output using the original model\n",
        "generation_config = GenerationConfig(max_new_tokens=150 , num_beams = 1)\n",
        "output_original_model = original_model.generate(input_ids=input_ids, generation_config=generation_config)\n",
        "original_model_text_output = tokenizer.decode(output_original_model[0], skip_special_tokens=True)\n",
        "\n",
        "# Generate output using the trained model\n",
        "output_trained_model = trained_model.generate(input_ids=input_ids, generation_config=generation_config)\n",
        "trained_model_text_output = tokenizer.decode(output_trained_model[0], skip_special_tokens=True)\n",
        "\n",
        "human_baseline_summary = summary\n",
        "dash_line = '-' * 50\n",
        "print(dash_line)\n",
        "print(f'BASELINE HUMAN SUMMARY:\\n{human_baseline_summary}')\n",
        "print(dash_line)\n",
        "print(f'ORIGINAL MODEL:\\n{original_model_text_output}')\n",
        "print(dash_line)\n",
        "print(f'TRAINED MODEL:\\n{trained_model_text_output}')"
      ],
      "metadata": {
        "id": "pyhEioM2hQYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install rouge_score\n",
        "rouge = evaluate.load('rouge')\n"
      ],
      "metadata": {
        "id": "dh37Tk3HXyNf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dialogue = dataset['test'][0:10]['dialogue']\n",
        "human_baseline_summaries = dataset['test'][0:10]['summary']\n",
        "\n",
        "original_model_summaries = []\n",
        "instruct_model_summaries = []\n",
        "\n",
        "for _ , dialogue in enumerate(dialogue):\n",
        "  prompt = f\"\"\"\n",
        "  summarize the following text\"\"\"\n",
        "  input_ids = tokenizer(prompt , return_tensors = 'pt').input_ids\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  input_ids = input_ids.to(device)\n",
        "\n",
        "\n",
        "    # Generate output using the original model\n",
        "  generation_config = GenerationConfig(max_new_tokens=150 , num_beams = 1)\n",
        "  output_original_model = original_model.generate(input_ids=input_ids, generation_config=generation_config)\n",
        "  original_model_text_output = tokenizer.decode(output_original_model[0], skip_special_tokens=True)\n",
        "  original_model_summaries.append(original_model_text_output)\n",
        "\n",
        "# Generate output using the trained model\n",
        "  instruct_model_outputs = trained_model.generate(input_ids=input_ids, generation_config=generation_config)\n",
        "  instruct_model_text_output = tokenizer.decode(output_trained_model[0], skip_special_tokens=True)\n",
        "  instruct_model_summaries.append(instruct_model_text_output)\n",
        "\n",
        "zipped_summaries = list(zip(human_baseline_summaries , original_model_summaries , instruct_model_summaries))\n",
        "df = pandas.DataFrame(zipped_summaries , columns = ['human_baseline_summaries' , 'original_model_summaries' , 'instruct_model_summaries'])\n",
        "df"
      ],
      "metadata": {
        "id": "T_OlRiakaErF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from rouge import Rouge\n",
        "rouge = Rouge()\n",
        "original_model_results = rouge.get_scores(\n",
        "    original_model_summaries,\n",
        "    human_baseline_summaries[0:len(original_model_summaries)],\n",
        "    )\n",
        "instruct_model_results = rouge.get_scores(\n",
        "    instruct_model_summaries,\n",
        "    human_baseline_summaries[0:len(instruct_model_summaries)],\n",
        "    )\n",
        "print('ORIGINAL MODEL:')\n",
        "print(original_model_results)\n",
        "print('INSTRUCT MODEL:')\n",
        "print(instruct_model_results)"
      ],
      "metadata": {
        "id": "CsTDAZeNaE3Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install peft"
      ],
      "metadata": {
        "id": "HUF_b5uNfgYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig , get_peft_model , TaskType"
      ],
      "metadata": {
        "id": "Qh2BgzDifksL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=32, # Rank\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q\", \"v\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.SEQ_2_SEQ_LM # FLAN-T5\n",
        ")"
      ],
      "metadata": {
        "id": "XC6QWHQUfvTB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "peft_model= get_peft_model(\n",
        "    original_model,\n",
        "    lora_config\n",
        ")\n",
        "print(print_number_of_model_trained_parameters(peft_model\n",
        "                                               ))"
      ],
      "metadata": {
        "id": "B-1pJw2MeTPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_dir = f'./dialogue-summary-training-{str(int(time.time()))}'\n",
        "peft_training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    auto_find_batch_size=True,\n",
        "    learning_rate=1e-3, # Higher learning rate than full fine-tuning.\n",
        "    num_train_epochs=1,\n",
        "    logging_steps=1,\n",
        ")\n",
        "\n",
        "peft_trainer = Trainer(\n",
        "    model=peft_model,\n",
        "    args=peft_training_args,\n",
        "    train_dataset=tokenized_dataset[\"train\"],\n",
        ")"
      ],
      "metadata": {
        "id": "g9qmOqTuery9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "peft_trainer.train()\n",
        "peft_model_path=\"./peft-dialogue-summary-checkpoint/\"\n",
        "peft_trainer.model.save_pretrained(peft_model_path)\n",
        "tokenizer.save_pretrained(peft_model_path)\n"
      ],
      "metadata": {
        "id": "d9GbcTihf-cq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import peft_model , PeftConfig\n",
        "path_base_model = \"google/flan-t5-base\"\n",
        "peft_model_base = AutoModelForSeq2SeqLM.from_pretrained(path_base_model , torch_dtype  = torch.bfloat16)\n",
        "tokenizer = AutoTokenizer.from_pretrained(path_base_model)\n",
        "peft_model = PeftModel.from_pretrained(peft_model_base , peft_model_path)"
      ],
      "metadata": {
        "id": "0GgQfVsPhU6T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index = 150\n",
        "dialogue = dataset['test'][index]['dialogue']\n",
        "base_line_human_summary = dataset['test'][index]['summary']\n",
        "\n",
        "prompt = f\"\"\"\n",
        "Summarize the following conversation\n",
        "\n",
        "{dialogue}\n",
        "\n",
        "Summary:\n",
        "\"\"\"\n",
        "\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "\n",
        "# Ensure that input_ids and the models are on the same device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "input_ids = input_ids.to(device)\n",
        "original_model.to(device)\n",
        "trained_model.to(device)\n",
        "peft_model.to(device)\n",
        "\n",
        "original_model_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
        "original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
        "print(original_model_text_output)\n",
        "\n",
        "instruct_model_outputs = trained_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
        "instruct_model_text_output = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n",
        "\n",
        "peft_model_outputs = peft_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
        "peft_model_text_output = tokenizer.decode(peft_model_outputs[0], skip_special_tokens=True)\n",
        "\n",
        "print(dash_line)\n",
        "print(f'BASELINE HUMAN SUMMARY: \\n{base_line_human_summary}')\n",
        "print(dash_line)\n",
        "print(f'ORIGINAL MODEL: \\n{original_model_text_output}')\n",
        "print(dash_line)\n",
        "print(f'TRAINED MODEL: \\n{instruct_model_text_output}')\n",
        "print(dash_line)\n",
        "print(f'PEFT MODEL: \\n{peft_model_text_output}')"
      ],
      "metadata": {
        "id": "21SetQahiYAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dialogues = dataset['test'][0:10]['dialogue']\n",
        "human_baseline_summaries = dataset['test'][0:10]['summary']\n",
        "original_model_summaries = []\n",
        "instruct_model_summaries = []\n",
        "peft_model_summaries = []\n",
        "\n",
        "for idx, dialogue in enumerate(dialogues):\n",
        "  prompt = f\"\"\"\n",
        "  summarize the following conversation\n",
        "  {dialogue}\n",
        "  Summary:\n",
        "\n",
        "  \"\"\"\n",
        "  input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "\n",
        "  # Ensure that input_ids and the models are on the same device\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  input_ids = input_ids.to(device)\n",
        "\n",
        "  human_baseline_text_output = human_baseline_summaries[idx]\n",
        "\n",
        "  original_model_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
        "  original_model_text_output = tokenizer.decode(original_model_outputs[0], skip_special_tokens=True)\n",
        "  original_model_summaries.append(original_model_text_output)\n",
        "\n",
        "  instruct_model_outputs = original_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
        "  instruct_model_text_output = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n",
        "  instruct_model_summaries.append(instruct_model_text_output)\n",
        "\n",
        "  peft_model_outputs = peft_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
        "  peft_model_text_output = tokenizer.decode(peft_model_outputs[0], skip_special_tokens=True)\n",
        "  peft_model_summaries.append(peft_model_text_output)\n",
        "\n",
        "zipped_summaries = list(zip(human_baseline_summaries, original_model_summaries, instruct_model_summaries))\n",
        "\n",
        "df = pd.DataFrame(zipped_summaries, columns=['human_baseline_summaries', 'original_model_summaries', 'instruct_model_summaries'])\n",
        "df"
      ],
      "metadata": {
        "id": "9TumRVR1in1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rouge = evaluate.load('rouge')\n",
        "\n",
        "original_model_results = rouge.compute(\n",
        "    predictions=original_model_summaries,\n",
        "    references=human_baseline_summaries[0:len(original_model_summaries)],\n",
        "    use_aggregator=True,\n",
        "    use_stemmer=True,\n",
        ")\n",
        "\n",
        "instruct_model_results = rouge.compute(\n",
        "    predictions=instruct_model_summaries,\n",
        "    references=human_baseline_summaries[0:len(instruct_model_summaries)],\n",
        "    use_aggregator=True,\n",
        "    use_stemmer=True,\n",
        ")\n",
        "\n",
        "peft_model_results = rouge.compute(\n",
        "    predictions=peft_model_summaries,\n",
        "    references=human_baseline_summaries[0:len(peft_model_summaries)],\n",
        "    use_aggregator=True,\n",
        "    use_stemmer=True,\n",
        ")\n",
        "\n",
        "print('ORIGINAL MODEL:')\n",
        "print(original_model_results)\n",
        "print('INSTRUCT MODEL:')\n",
        "print(instruct_model_results)\n",
        "print('PEFT MODEL:')\n",
        "print(peft_model_results)"
      ],
      "metadata": {
        "id": "1BxHFSMsi7Z-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E7s5aBMElFuc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}